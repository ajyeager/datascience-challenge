{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final report for challenge:\n",
    "# https://docs.google.com/document/d/1zvWnFWvYVeoPrWs97bt5d9E2DJiMhW87jhdRkRYvi_E/edit#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import load_data\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "\n",
    "from pymagnitude import Magnitude\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import numpy as np\n",
    "import business_classification\n",
    "import rating_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/Users/vijay/Documents/vijay/yelp-dataset\"\n",
    "reviews_file = data_dir + \"/yelp_academic_dataset_review.json\"\n",
    "fasttext_data_dir = \"/Users/vijay/Documents/vijay/datascience-challenge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for row in open(reviews_file):\n",
    "    review = json.loads(row)\n",
    "    reviews.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Rating Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_data.train_test_split(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_out = fasttext_data_dir + \"/train.fasttext\"\n",
    "load_data.preprocess_for_fasttext(train_data, train_out)\n",
    "\n",
    "test_out = fasttext_data_dir + \"/test.fasttext\"\n",
    "load_data.preprocess_for_fasttext(test_data, test_out)\n",
    "rating_prediction.prepare_fast_text_data(fasttext_data_dir, train, test)\n",
    "\n",
    "test_labels_out = fasttext_data_dir + \"/test.fasttext.label_only\"\n",
    "test_labels_only = open(test_labels_out)\n",
    "for review in test_data:\n",
    "    label = review['stars']\n",
    "    # fast text expects the label formatted as __label__1 for label 1\n",
    "    annotated_line = \"__label__%s %s\\n\" % (label, text)\n",
    "    test_labels_only.write(annotated_line)\n",
    "    lines += 1\n",
    "    \n",
    "test_labels_only.close()\n",
    "\n",
    "print(\"now `cd %s`\\n\" % fasttext_data_dir)\n",
    "print(\"and then run `../fastText-0.1.0/fasttext supervised -input train.fasttext -output model` to train the model\")\n",
    "print(\"and `../fastText-0.1.0/fasttext predict model.bin test.fasttext > test.predictions` to write predictions to file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline A: predict the most frequent label:\n",
    "train_labels = [review['stars'] for review in train]\n",
    "actual_labels = [review['stars'] for review in test]\n",
    "\n",
    "# Baseline: what if we just chose the most frequently seen business every time?\n",
    "top_labels = Counter(actual_labels)\n",
    "[(most_frequent_label, _)] = top_labels.most_common(1)\n",
    "choose_most_frequent = np.repeat(most_frequent_label, len(actual_labels))\n",
    "print(\"Test-set accuracy of blindly choosing the most frequent rating (which is %s): %s\" %\n",
    "      (most_frequent_label, sum(choose_most_frequent == actual_labels) / len(actual_labels)))\n",
    "# 0.441\n",
    "\n",
    "print(\"Test-set RMSE of blindly choosing the most frequent rating (which is %s): %s\" %\n",
    "      (most_frequent_label, rating_prediction.rmse(choose_most_frequent, actual_labels)))\n",
    "# \n",
    "\n",
    "threes = np.repeat(3, len(actual_labels))\n",
    "print(\"Test-set accuracy of blindly choosing rating 3: %s\" % (sum(threes == actual_labels) / len(actual_labels)))\n",
    "# 0.441\n",
    "\n",
    "print(\"Test-set RMSE of blindly choosing rating 3: %s\" % rating_prediction.rmse(threes, actual_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second approach: native Fasttext classification:\n",
    "    \n",
    "fasttext_data_dir = \"/Users/vijay/Documents/vijay/datascience-challenge\"\n",
    "predicted_labels = fasttext_data_dir + \"/test_predicted_fasttext5\"\n",
    "test_labels_fname = fasttext_data_dir + \"/test.fasttext.label_only\"\n",
    "\n",
    "acc, rmse = rating_prediction.evaluate_fasttext_predictions(predicted_labels, test_labels_fname)\n",
    "print(\"accuracy:\", acc)\n",
    "# 0.6935579079926016\n",
    "print(\"rmse:\", rmse)\n",
    "# 0.7518566049250488"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "label_prefix = \"__label__\"\n",
    "remove_len = len(label_prefix)\n",
    "for label_row in open(predicted_labels):\n",
    "    predicted.append(int(label_row[remove_len:]))\n",
    "\n",
    "test_labels = [row['stars'] for row in test]\n",
    "    \n",
    "print(sum(np.asarray(predicted) == np.asarray(test_labels)) / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute features for each review (i.e. average the fasttext word embeddings for each token)\n",
    "# and write to file\n",
    "vecs_dir = \"/Users/vijay/Documents/vijay/datascience-challenge/vecs\"\n",
    "\n",
    "rating_prediction.compute_word_embeddings(train, vecs_dir, data_prefix=\"train\", batch_write_size=10000)\n",
    "rating_prediction.compute_word_embeddings(test, vecs_dir, data_prefix=\"test\", batch_write_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_dir = \"/Users/vijay/Documents/vijay/datascience-challenge/vecs\"\n",
    "train_features = np.loadtxt(vecs_dir + \"/train_features\")\n",
    "train_labels = np.loadtxt(open(vecs_dir + \"/train_labels\"))\n",
    "test_features = np.loadtxt(vecs_dir + \"/test_features\")\n",
    "test_labels = np.loadtxt(open(vecs_dir + \"/test_labels\"))\n",
    "\n",
    "# A very small number of reviews resulted in word-embedding matrices with NaN values\n",
    "# I investigated these reviews on a case-by-case basis and couldn't identify any obvious issues in the raw text,\n",
    "# so we just ignore these rows from the train and test sets here.\n",
    "\n",
    "train_nan_indices = [x[0] for x in np.argwhere(np.isnan(train_features))]\n",
    "X = np.delete(train_features, train_nan_indices, axis=0)\n",
    "y = np.delete(train_labels, train_nan_indices, axis=0)\n",
    "\n",
    "test_nan_indices = [x[0] for x in np.argwhere(np.isnan(test_features))]\n",
    "X_test = np.delete(test_features, test_nan_indices, axis=0)\n",
    "y_test = np.delete(test_labels, test_nan_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X,y)\n",
    "\n",
    "linear_predictions = np.asarray(linear.predict(X_test)\n",
    "linear_discretized = rating_prediction.round_regressor_predictions(linear_predictions)\n",
    "print(\"linear regression - classification accuracy:\", sum(linear_discretized == y_test) / len(y_test))\n",
    "# 0.365\n",
    "print(\"linear regression - classification RMSE:\", rating_prediction.rmse(linear_discretized, y_test) / len(y_test))\n",
    "# 1.055\n",
    "print(\"linear regression - regression RMSE:\", rating_prediction.rmse(linear_predictions, y_test) / len(y_test))\n",
    "# 1.110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "logr.fit(X,y)\n",
    "\n",
    "logr_predictions = logr.predict(X_test)\n",
    "print(\"logistic regression - classification accuracy:\", sum(logr_predictions == y_test) / len(y_test))\n",
    "# 0.630\n",
    "print(\"logistic regression - classification RMSE\", rating_prediction.rmse(logr_predictions, y_test) / len(y_test))\n",
    "# 1.058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Business Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load review data, group by business, and write data to disk in the FastText format, then train/evaluate model\n",
    "\n",
    "business_ids = [review[\"business_id\"] for review in reviews]\n",
    "top_businesses = []\n",
    "\n",
    "for review in reviews:\n",
    "    business_id = review[\"business_id\"]\n",
    "    top_businesses.append(business_id)\n",
    "    \n",
    "# get the top 100 most frequently-rated businesses\n",
    "top_businesses = Counter(top_businesses)\n",
    "top_100 = dict(top_businesses.most_common(100))\n",
    "\n",
    "reviews_by_business = defaultdict(list)\n",
    "reviews_top_businesses = []\n",
    "for i, review in enumerate(reviews):\n",
    "    if review[\"business_id\"] in top_100:\n",
    "        # reviews_by_business[business_id].append(text)\n",
    "        reviews_top_businesses.append(review)\n",
    "\n",
    "train_business_reviews, test_business_reviews = load_data.train_test_split(reviews_top_businesses)\n",
    "fasttext_data_dir = \"/Users/vijay/Documents/vijay/datascience-challenge\"\n",
    "load_data.preprocess_for_fasttext(train_business_reviews, fasttext_data_dir + \"/train.businesses.fasttext\", label_key = 'business_id')\n",
    "load_data.preprocess_for_fasttext(test_business_reviews, fasttext_data_dir + \"/test.businesses.fasttext\", label_key = 'business_id')\n",
    "\n",
    "test_labels_out = fasttext_data_dir + \"/test.business.fasttext.label_only\"\n",
    "test_labels_only = open(test_labels_out)\n",
    "for review in test_data:\n",
    "    label = review['business_id']\n",
    "    # fast text expects the label formatted as __label__1 for label 1\n",
    "    annotated_line = \"__label__%s %s\\n\" % (label, text)\n",
    "    test_labels_only.write(annotated_line)\n",
    "    lines += 1\n",
    "    \n",
    "test_labels_only.close()\n",
    "\n",
    "print(\"now run `../fastText-0.1.0/fasttext supervised -input train.businesses.fasttext -output model.businesses -pretrainedVectors model.vec`.\")\n",
    "print(\"to train the model predicting the business id given review text, then:\"\n",
    "print(\"../fastText-0.1.0/fasttext predict model.businesses.bin test.businesses.fasttext > train.business.predictions to make predictions against the test data.\"\n",
    "print(\"True test labels were written to %s\" % (fasttext_data_dir + \"/test.business.fasttext.label_only\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating multi-review (e.g. 1, 5, 10 reviews) group predictions\n",
    "actual_businesses = np.asarray(open('test.business.fasttext.label_only').readlines())\n",
    "actual_business_ids = [load_data.trim_label(label) for label in actual_businesses]\n",
    "\n",
    "# Baseline: what if we just chose the most frequently seen business every time?\n",
    "[(most_frequent_label, _)] = top_businesses.most_common(1)\n",
    "choose_most_frequent = np.repeat(most_frequent_label, len(actual)\n",
    "print(\"Test-set accuracy of blindly choosing the most frequent of the 100 businessses:\",\n",
    "      sum(choose_most_frequent == actual_business_ids) / len(actual_business_ids))\n",
    "# 0.0297 accuracy on test set (while picking a random label would give a true accuracy of 0.01)\n",
    "                                 \n",
    "# Fasttext model                               \n",
    "predicted_businesses = np.asarray(open('train.business.predictions').readlines())\n",
    "\n",
    "# Evaluating group predictions\n",
    "test_groups_of_1 = business_classification.group_reviews(test_business_reviews, group_size=1)\n",
    "predicted, actual = business_classification.make_group_predictions(test_groups_of_1, predicted_businesses, fallback=None)\n",
    "group_accuracy = sum(predicted == actual) / len(actual)\n",
    "print(\"Test-set accuracy of FastText business-prediction model, voting over groups of 5: %s over %s test groups\", group_accuracy, len(actual))\n",
    "# 0.593 accuracy on test set\n",
    "                \n",
    "                                 \n",
    "test_groups_of_5 = business_classification.group_reviews(test_business_reviews, group_size=5)\n",
    "predicted, actual = business_classification.make_group_predictions(test_groups_of_5, predicted_businesses, fallback=most_frequent_label)\n",
    "group_accuracy = sum(predicted == actual) / len(actual)\n",
    "print(\"Test-set accuracy of FastText business-prediction model, voting over groups of 5: %s over %s test groups\", group_accuracy, len(actual))\n",
    "# 0.645 accuracy on test set\n",
    "                            \n",
    "test_groups_of_10 = business_classification.group_reviews(test_business_reviews, group_size=10)\n",
    "predicted, actual = business_classification.make_group_predictions(test_groups_of_10, predicted_businesses, fallback=most_frequent_label)\n",
    "sum(predicted == actual) / len(actual)\n",
    "print(\"Test-set accuracy of FastText business-prediction model, voting over groups of 10: %s over %s test groups\", group_accuracy, len(actual))\n",
    "# 0.827 accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experimental\n",
    "# Nearest-neighbor search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
